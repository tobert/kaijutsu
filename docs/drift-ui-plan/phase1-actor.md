# Phase 1: Extend ActorHandle

**Goal:** Make ActorHandle the complete RPC surface for the Bevy app — macro-generated methods, subscription broadcast, and full Tier 1 coverage.

## Key Files

| File | Action |
|------|--------|
| `crates/kaijutsu-client/src/actor.rs` | Extend with macro + new methods (510 → ~700 lines) |
| `crates/kaijutsu-client/src/subscriptions.rs` | **New** — ServerEvent enum + broadcast infrastructure |
| `crates/kaijutsu-client/src/rpc.rs` | Reference only (KernelHandle methods we're wrapping) |
| `crates/kaijutsu-client/src/lib.rs` | Re-export subscriptions module |

## Architecture Decisions

### 1. `actor_rpc!` Macro

The current pattern for adding an ActorHandle method requires touching three places:

1. `RpcCommand` enum variant (with reply oneshot)
2. `impl ActorHandle` pub method (sends command, awaits reply)
3. `RpcActor::run()` match arm (calls KernelHandle, sends reply)

This is 100% mechanical. A declarative macro generates all three from one line:

```rust
// Invocation:
actor_rpc! {
    /// Push content to another context via drift
    drift_push(target_ctx: &str, content: &str, summarize: bool) -> u64;

    /// Execute a prompt and return the prompt ID
    prompt(content: &str, model: Option<&str>, cell_id: &str) -> String;
}

// Expansion for each line:
// 1. enum RpcCommand { DriftPush { target_ctx: String, content: String, summarize: bool, reply: oneshot::Sender<Result<u64, ActorError>> } }
// 2. impl ActorHandle { pub async fn drift_push(&self, target_ctx: &str, content: &str, summarize: bool) -> Result<u64, ActorError> { ... } }
// 3. match arm in dispatch: RpcCommand::DriftPush { target_ctx, content, summarize, reply } => { rpc_call!(...) }
```

**Design notes:**
- Parameters are `&str` in the method signature, cloned to `String` in the enum variant
- Return type `T` becomes `Result<T, ActorError>` in reply channel
- Method names are snake_case, enum variants are PascalCase (auto-converted)
- The macro doesn't handle subscriptions (those use a different pattern)

**Macro structure:** The macro should generate a standalone `dispatch()` function rather than
trying to inject match arms into the handwritten `RpcActor::run()` loop. The loop calls
`dispatch(cmd, &kernel, &err_tx)` and the macro owns the entire match:

```rust
// Generated by actor_rpc!
async fn dispatch(cmd: RpcCommand, kernel: &KernelHandle, err_tx: &UnboundedSender<()>) {
    match cmd {
        RpcCommand::DriftPush { target_ctx, content, summarize, reply } => {
            rpc_call!(kernel, reply, err_tx, k, k.drift_push(&target_ctx, &content, summarize));
        }
        // ... one arm per macro line
    }
}
```

This avoids the "spooky action at a distance" problem of a macro injecting code into a
manually written loop. The run loop stays readable, the dispatch logic is self-contained,
and adding a new RPC method is still a one-liner in the macro invocation.

### 2. Subscription Architecture

Subscriptions (block events, resource events, elicitation events) are long-lived server-to-client streams. They don't fit the request/response pattern of `rpc_call!`.

**Design: `broadcast::Sender<ServerEvent>`**

```rust
// subscriptions.rs

/// Events pushed from the server to the app
#[derive(Clone, Debug)]
pub enum ServerEvent {
    // Block events (from BlockEvents callback)
    BlockInserted { document_id: String, block: BlockSnapshot, ops: Vec<u8> },
    BlockTextOps { document_id: String, block_id: BlockId, ops: Vec<u8> },
    BlockStatusChanged { document_id: String, block_id: BlockId, status: Status },
    BlockDeleted { document_id: String, block_id: BlockId },
    BlockCollapsedChanged { document_id: String, block_id: BlockId, collapsed: bool },
    BlockMoved { document_id: String, block_id: BlockId, after_id: Option<BlockId> },

    // MCP resource events
    ResourceUpdated { server: String, uri: String, contents: Option<McpResourceContents> },
    ResourceListChanged { server: String, resources: Option<Vec<McpResource>> },

    // MCP tool results (async)
    McpToolResult { server: String, tool: String, result: Result<Value, String> },
}

/// Connection lifecycle events (separate from server events)
#[derive(Clone, Debug)]
pub enum ConnectionStatus {
    Connected,
    Disconnected,
    Reconnecting { attempt: u32 },
    Error(String),
}
```

**Flow:**
1. `RpcActor` owns `broadcast::Sender<ServerEvent>` (capacity 256) and `broadcast::Sender<ConnectionStatus>` (capacity 16)
2. `ActorHandle` exposes `pub fn subscribe_events(&self) -> broadcast::Receiver<ServerEvent>` and `pub fn subscribe_status(&self) -> broadcast::Receiver<ConnectionStatus>`
3. On connect, `RpcActor` calls `kernel.subscribe_blocks()` and `kernel.subscribe_mcp_resources()` with callbacks that forward to the broadcast sender
4. On reconnect, subscriptions are **explicitly re-registered** (see below)
5. Broadcast lag triggers document resync (see below)

**Why broadcast, not mpsc:**
- Multiple Bevy systems need block events (cell/systems.rs, timeline, constellation)
- `broadcast` supports multiple receivers with independent cursors
- Lagging receivers get `RecvError::Lagged(n)` instead of blocking the sender

### ⚠️ Broadcast Lag Recovery (Critical)

Block events (`BlockInserted`, `BlockTextOps`, etc.) are **not** best-effort — they carry CRDT
operations. Missing a single op corrupts the local document state permanently. The broadcast
channel has fixed capacity; if the UI thread stalls (window resize, heavy asset load, debug
build), the channel fills and `try_recv()` returns `RecvError::Lagged(n)`.

**Recovery strategy: Generation counter**

Rather than one coarse `DocumentsStale` event that invalidates everything, use a generation
counter so documents can detect and recover from staleness individually:

```rust
// In subscriptions.rs:
/// Monotonic generation counter — bumped on lag or reconnect
pub struct SyncGeneration(pub u64);

// In CachedDocument (Phase 4):
pub struct CachedDocument {
    pub document: BlockDocument,
    pub sync_version: u64,
    pub context_name: String,
    pub last_accessed: Instant,
    pub synced_at_generation: u64,  // generation when last synced
}
```

```rust
// In poll_server_events (Phase 2's ActorPlugin):
fn poll_server_events(
    mut generation: ResMut<SyncGeneration>,
    // ...
) {
    loop {
        match receiver.try_recv() {
            Ok(event) => events.write(ServerEventMessage(event)),
            Err(TryRecvError::Empty) => break,
            Err(TryRecvError::Lagged(n)) => {
                log::warn!("ServerEvent broadcast lagged by {n} events, bumping generation");
                generation.0 += 1;
                break;
            }
            Err(TryRecvError::Closed) => break,
        }
    }
}
```

**How documents resync:**
1. Active document: when `generation > doc.synced_at_generation`, immediately call
   `get_document_state()` to re-fetch full state, then update `synced_at_generation`
2. Background documents (Phase 4): detected as stale on access — when switching to a
   cached doc, compare its `synced_at_generation` against current generation. If behind,
   re-fetch before displaying. Same code path as a cache miss.
3. Generation also bumps on reconnect (see below), so reconnect + lag use the same mechanism

**Why generation counter, not per-document tracking:**
- Broadcast lag doesn't tell us *which* documents were affected (events are interleaved)
- But it's cheap to check — one integer comparison per document access
- Avoids the thundering herd of resyncing 8 docs simultaneously on lag
- Active doc resyncs immediately; background docs resync lazily on next switch

**Sizing the channel:** 256 slots should be generous for normal operation (block events arrive
at ~10-50/sec during active streaming). With 5-10 concurrent agents, peak could reach
~200-500 events/sec — monitor lag frequency in practice and increase if needed.

### Reconnection: Subscription Re-registration

The current `RpcActor::ensure_connected()` establishes the SSH connection and gets a `KernelHandle`,
but it does **not** automatically re-subscribe to block events or resource events. Subscriptions
are stateful server-side — a new `KernelHandle` starts with no active subscriptions.

**On reconnect, `RpcActor` must explicitly:**
1. Call `kernel.subscribe_blocks(callback)` with a fresh callback forwarding to the broadcast sender
2. Call `kernel.subscribe_mcp_resources(callback)` likewise
3. Call `kernel.subscribe_mcp_elicitations(callback)` if needed
4. Bump the `SyncGeneration` counter — the client has missed events during the disconnect window

Wire this into `ensure_connected()` or a post-connect hook in the actor loop. The sequence is:
`connect → subscribe_blocks → subscribe_resources → bump generation → emit Connected`

The generation bump uses the same mechanism as broadcast lag recovery — the app-side
`poll_server_events` system detects the bumped generation and triggers resync for the active
document. Background documents resync lazily on next switch.

### 3. World-Level Methods

The current `ActorHandle` only wraps `Kernel` methods. Two `World` methods are needed:
- `whoami()` → Identity
- `list_kernels()` → Vec<KernelInfo>

These require a `WorldHandle` (the Cap'n Proto `World` capability). Options:

**Option A (preferred): Separate `WorldHandle` on ActorHandle**
```rust
pub struct ActorHandle {
    cmd_tx: mpsc::Sender<RpcCommand>,
    event_rx: broadcast::Sender<ServerEvent>,    // new
    status_rx: broadcast::Sender<ConnectionStatus>, // new
}

// World-level commands get their own RpcCommand variants
// RpcActor holds both World cap and Kernel cap
```

**Option B: WorldActorHandle**
Separate actor for world-level operations. Overkill — there are only 6 world methods and most are one-shot (whoami, list_kernels, attach_kernel, create_kernel, take_seat, list_my_seats).

Go with Option A. The `RpcActor` already manages connection state and can hold both capabilities.

## Implementation Steps

### Step 1: Define `actor_rpc!` macro
- Write the macro in `actor.rs` (or a `macros.rs` if it gets complex)
- Migrate existing 11 methods to use the macro
- Verify nothing breaks with `cargo check -p kaijutsu-client`

### Step 2: Create `subscriptions.rs`
- Define `ServerEvent` and `ConnectionStatus` enums, plus `SyncGeneration` counter
- Add `broadcast::Sender` fields to `RpcActor`
- Add `subscribe_events()` and `subscribe_status()` to `ActorHandle`
- Wire up `BlockEventsCallback` to forward to broadcast
- Wire up `ResourceEventsCallback` to forward to broadcast
- **Re-register subscriptions on reconnect:** add post-connect hook in `ensure_connected()` that calls `subscribe_blocks`, `subscribe_mcp_resources`, and bumps `SyncGeneration`

### Step 3: Add Tier 1 methods via macro
New methods (each one line in the macro invocation):
```
prompt(content: &str, model: Option<&str>, cell_id: &str) -> String
shell_execute(command: &str, cell_id: &str) -> BlockId
call_mcp_tool(server: &str, tool: &str, args: &str) -> McpToolResult
fork_from_version(document_id: &str, version: u64, context_name: &str) -> Context
cherry_pick_block(block_id: &BlockId, target_context: &str) -> BlockId
list_contexts() -> Vec<Context>
join_context(context_name: &str, instance: &str) -> SeatHandle
create_context(name: &str) -> Context
get_info() -> KernelInfo
```

### Step 4: Add World-level methods
- Extend `RpcCommand` with `Whoami`, `ListKernels`, `AttachKernel`, `CreateKernel`, `TakeSeat`, `ListMySeats`
- These use the World capability instead of Kernel
- `attach_kernel` is special — it sets the kernel capability on the actor

### Step 5: Connection lifecycle
- Emit `ConnectionStatus::Connected` after successful connection
- Emit `ConnectionStatus::Disconnected` on connection loss
- Emit `ConnectionStatus::Reconnecting` during retry loop
- Emit `ConnectionStatus::Error` on fatal errors

## Verification

- [ ] `cargo check -p kaijutsu-client` passes
- [ ] `cargo test -p kaijutsu-client` passes
- [ ] Existing drift e2e tests still pass (they exercise ActorHandle)
- [ ] All 11 existing methods work identically after macro migration
- [ ] New methods compile and have correct signatures matching KernelHandle
- [ ] broadcast subscriber can receive block events in a test
- [ ] `RecvError::Lagged` bumps `SyncGeneration` (unit test with small channel capacity)
- [ ] Reconnection re-registers block and resource subscriptions
- [ ] Reconnection bumps `SyncGeneration` so client resyncs
- [ ] Active document detects stale generation and resyncs within one frame

## Dependencies

- None — this is the foundation phase

## Status Log

| Date | Status | Notes |
|------|--------|-------|
| | | |
